# GPT
This is a repository that shows how a GPT can be trained and fine-tuned from scratch. At the end of all of this training and fine-tuning. The GPT model can be a chatbot with personality and can generate responses more of that a human person would rather than what ChatGPT generates(where its messages state that it is an AI). 

Note: At first, this project was simply meant to be a way for training GPT2 on a single GPU, but I decided to scale it up in order to create a state of the art chatbot. The fine-tuning parts are still being worked on, and to show that, I will use a pretrained model due to GPT2 taking several days(or months on a single GPU). 


